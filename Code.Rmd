
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
# Useful libraries
library(tidyverse)
library(dplyr)
library(mltools)
library(data.table)
library(caret)
library(ROSE)
library(CustomerScoringMetrics)
library(pROC)
library(randomForest)
library(randomForestSRC)
library(randomForestExplainer)
library(C50)
library(party)
library(e1071)
library(splitstackshape)
library(mice)
library(conflicted)
library(FSelector)
library(DescTools)
library(tidyr)
library(missForest)
```

# Data Preparation

```{r loading data}

# Read in dataset
lead_data_raw <- read.csv("assignment_data.csv", stringsAsFactors = TRUE)

# View the summary output of the dataset
str(lead_data_raw)
summary(lead_data_raw)

```

The data dictionary for the variables in our data set is given below

 
Variable             | Description 

----------------------|-------------------------------- 

ID                    | customer identification number 

Gender                |gender of the customer 

Age                   | age of the customer in years 

Dependent             | whether the customer has a dependent or not 

Marital_Status        | marital state (1=married, 2=single, 0 = others) 

Region_Code           | code of the region for the customer 

Years_at_Residence    | the duration in the current residence (in years) 

Occupation            | occupation type of the customer 

Channel_Code          |acquisition channel code used to reach the customer when they opened their bank account  

Vintage               | the number of months that the customer has been associated with the company 

Credit_Product        | if the customer has any active credit product (home loan, personal loan, credit card etc.) 

Avg_Account_Balance   | average account balance for the customer in last 12 months 

Account_Type          |account type of the customer with categories Silver, Gold and Platinum 

Active                | if the customer is active in last 3 months 

Registration          | whether the customer has visited the bank for the offered product registration (1 = yes; 0 = no) 

Target                | whether the customer has purchased the product (1 = yes; 0 = no)  


 
## Removal of unused variable
```{r remove unused variable}
# drop useless variable "ID" from the full dataset
lead_data_raw$ID <- NULL
```

## Data Factorization
```{r Variable Factorization}

# Set the correct measurement levels for Gender, Dependent, Marital_Status, Registration and Target using lapply() and as.factor() functions
lead_data <- lead_data_raw

# First generate a vector to keep the column names
columns <- c("Dependent", "Marital_Status", "Registration", "Target")

# Set the correct measurement levels or data types
# Factorize the categorical variables
lead_data[columns] <- lapply(lead_data[columns], as.factor)

str(lead_data)
summary(lead_data)
```
## Inspection of Missing Values

```{r Check NAs}

# Only variable "Credit_Product" contains NA's
# Partition the lead_data to complete cases and incomplete cases
NA_data <- lead_data[!complete.cases(lead_data), ]
no_NA_data <- lead_data[complete.cases(lead_data), ]

# Inspect the distribution of "Target" in complete cases and incomplete cases
summary(no_NA_data)
## Incomplete cases: 0/1 - 4350/13918 (Target)
## Complete cases: 0/1 - 183087/18645 (Target)
## Should not just delete all rows containing missing values from the full dataset
## May contain useful information for predicting target variable

```

## Inspection of Abnormal Data

```{r Inspection of a unknown level of Dependent}
# Partition the lead_data according to the 3 levels of the variable "Dependent"
minus1_data <- lead_data[lead_data$Dependent == "-1", ]
zero_data <- lead_data[lead_data$Dependent == "0", ]
one_data <- lead_data[lead_data$Dependent == "1", ]

# Inspect distribution of "Target" in each level of "Dependent"
summary(minus1_data) #  Target (0:1 - 30:80) 0.375
summary(zero_data) # Target (0:1 - 93253:16205) 5.75
summary(one_data) # Target (0:1 - 94154:16270) 5.77
## Cannot just delete rows with "-1" entries in "Dependent" variable, or replace them with the mode of the "Dependent" variable

## How to deal with it:
### First replace the "-1" entries with NA. 
lead_data <- lead_data_raw 
lead_data$Dependent <- replace(lead_data$Dependent, lead_data$Dependent < 0, NA)

```

## Feature Engineering: Data Encoding & Data Factorization

```{r data encoding}

# Encode some nominal and ordinal variables (categorical)
lead_data$Gender <- recode(lead_data$Gender, "Female" = 0, "Male" = 1)

lead_data$Occupation <- recode(lead_data$Occupation, "Entrepreneur" = 0, "Salaried" = 1, "Self_Employed" = 2, "Other" = 3)

lead_data$Channel_Code <- recode(lead_data$Channel_Code, "X1" = 0, "X2" = 1, "X3" = 2, "X4" = 3)

lead_data$Credit_Product <- recode(lead_data$Credit_Product, "No" = 0, "Yes" = 1)

lead_data$Account_Type <- recode(lead_data$Account_Type, "Silver" = 0, "Gold" = 1, "Platinum" = 2)

lead_data$Active <- recode(lead_data$Active, "No" = 0, "Yes" = 1)

# Re-factorize the categorical variables
columns <- c("Gender", "Dependent", "Marital_Status", "Region_Code", "Occupation", "Channel_Code", "Credit_Product", "Account_Type", "Active", "Registration", "Target")
lead_data[columns] <- lapply(lead_data[columns], as.factor)
summary(lead_data)
```


## Dealing with Missing Values: Imputation Methods

```{r CART in MICE Imputation method}
lead_data <- mice(lead_data, method = 'cart', diagnostics = FALSE, remove_collinear = FALSE, seed = 123)
lead_data <- as.data.frame(complete(lead_data))
```


## Dealing with Data Imbalance

The response variable 'Target' exhibits a substantial class imbalance, with $85.2%$ of observations classified as '$0$' and only $14.8%$ as '$1$'. To address this imbalance, three sampling methods: Under-sampling, over-sampling, and a combination of both were employed across three ratios $(0.5, 0.4, 0.3)$, yielding a total of $9$ models. 

## Training-Test Split

We trialled several split ratios and concluded that splitting the training and test data, $70:30$ was most optimal, using the function ‘createDataPartition’ in the carat package.  

# Modelling
## LR Models
### Different Sampling Method

```{r 70 percent traning data}
# Partition the dataset into training (70%) and test (30%) sets. Do not forget to set the seed for random variables. 

# Set seed to 123
set.seed(123)

# Partition the dataset into training and test sets

# index_RF keeps the record indices for the training data (RF Imputed)
index = createDataPartition(lead_data$Target, p = 0.7, list = FALSE)
# Generate training and test data for the complete dataset with NA removed
training = lead_data[index,]
test = lead_data[-index,]

```

```{r Data balancing in Target variable}

# Apply over sampling, under sampling and over sampling techniques on the training data
# Try different balancing level (p-value: 0.5, 0.4, 0.3)

training_both0.5 <- ovun.sample(Target ~ ., data = training, method = "both", p=0.5, seed=1)$data

training_under0.5 <- ovun.sample(Target ~ ., data = training, method = "under", p=0.5, seed=1)$data

training_over0.5 <- ovun.sample(Target ~ ., data = training, method = "over", p=0.5, seed=1)$data

training_both0.4 <- ovun.sample(Target ~ ., data = training, method = "both", p=0.4, seed=1)$data

training_under0.4 <- ovun.sample(Target ~ ., data = training, method = "under", p=0.4, seed=1)$data

training_over0.4 <- ovun.sample(Target ~ ., data = training, method = "over", p=0.4, seed=1)$data

training_both0.3 <- ovun.sample(Target ~ ., data = training, method = "both", p=0.3, seed=1)$data

training_under0.3 <- ovun.sample(Target ~ ., data = training, method = "under", p=0.3, seed=1)$data

training_over0.3 <- ovun.sample(Target ~ ., data = training, method = "over", p=0.3, seed=1)$data

```


```{r both sampling method and 0.5 p-value}

# Use function information.gain to compute information gain values of the attributes
weights <- information.gain(Target ~ ., data = training_both0.5)

# Print weights
print(weights)

# add row names as a column to keep them during ordering
weights$attr  <- rownames(weights)

# Let's sort the weights in decreasing order of information gain values.
# We will use arrange() function 
weights <- arrange(weights, -weights$attr_importance) 

# Plot the weights
barplot(weights$attr_importance, names = weights$attr, las = 2, ylim = c(0, 0.2))

# Filter features where the information gain is not zero
both0.5_features <- mice::filter(weights, attr_importance > 0)$attr

# Use cutoff.biggest.diff() 
# This function returns the attributes that have a significantly higher information gain values than the others.
#cutoff.biggest.diff(weights)

```

```{r under sampling and 0.5 p-value}

# Use function information.gain to compute information gain values of the attributes
weights <- information.gain(Target ~ ., data = training_under0.5)

# Print weights
print(weights)

# add row names as a column to keep them during ordering
weights$attr  <- rownames(weights)

# Let's sort the weights in decreasing order of information gain values.
# We will use arrange() function 
weights <- arrange(weights, -weights$attr_importance) 

# Plot the weights
barplot(weights$attr_importance, names = weights$attr, las = 2, ylim = c(0, 0.2))

# Filter features where the information gain is not zero
under0.5_features <- mice::filter(weights, attr_importance > 0)$attr

# Use cutoff.biggest.diff() 
# This function returns the attributes that have a significantly higher information gain values than the others.
#cutoff.biggest.diff(weights)

```
 

```{r over sampling and 0.5 p-value}

# Use function information.gain to compute information gain values of the attributes
weights <- information.gain(Target ~ ., data = training_over0.5)

# Print weights
print(weights)

# add row names as a column to keep them during ordering
weights$attr  <- rownames(weights)

# Let's sort the weights in decreasing order of information gain values.
# We will use arrange() function 
weights <- arrange(weights, -weights$attr_importance) 

# Plot the weights
barplot(weights$attr_importance, names = weights$attr, las = 2, ylim = c(0, 0.2))

# Filter features where the information gain is not zero
over0.5_features <- mice::filter(weights, attr_importance > 0)$attr

# Use cutoff.biggest.diff() 
# This function returns the attributes that have a significantly higher information gain values than the others.
#cutoff.biggest.diff(weights)

```

```{r both sampling and 0.4 p-value}

# Use function information.gain to compute information gain values of the attributes
weights <- information.gain(Target ~ ., data = training_both0.4)

# Print weights
print(weights)

# add row names as a column to keep them during ordering
weights$attr  <- rownames(weights)

# Let's sort the weights in decreasing order of information gain values.
# We will use arrange() function 
weights <- arrange(weights, -weights$attr_importance) 

# Plot the weights
barplot(weights$attr_importance, names = weights$attr, las = 2, ylim = c(0, 0.2))

# Filter features where the information gain is not zero
both0.4_features <- mice::filter(weights, attr_importance > 0)$attr

# Use cutoff.biggest.diff() 
# This function returns the attributes that have a significantly higher information gain values than the others.
#cutoff.biggest.diff(weights)

```

```{r under sampling and 0.4 p-value}

# Use function information.gain to compute information gain values of the attributes
weights <- information.gain(Target ~ ., data = training_under0.4)

# Print weights
print(weights)

# add row names as a column to keep them during ordering
weights$attr  <- rownames(weights)

# Let's sort the weights in decreasing order of information gain values.
# We will use arrange() function 
weights <- arrange(weights, -weights$attr_importance) 

# Plot the weights
barplot(weights$attr_importance, names = weights$attr, las = 2, ylim = c(0, 0.2))

# Filter features where the information gain is not zero
under0.4_features <- mice::filter(weights, attr_importance > 0)$attr

# Use cutoff.biggest.diff() 
# This function returns the attributes that have a significantly higher information gain values than the others.
cutoff.biggest.diff(weights)

```

```{r over sampling and 0.4 p-value}

# Use function information.gain to compute information gain values of the attributes
weights <- information.gain(Target ~ ., data = training_over0.4)

# Print weights
print(weights)

# add row names as a column to keep them during ordering
weights$attr  <- rownames(weights)

# Let's sort the weights in decreasing order of information gain values.
# We will use arrange() function 
weights <- arrange(weights, -weights$attr_importance) 

# Plot the weights
barplot(weights$attr_importance, names = weights$attr, las = 2, ylim = c(0, 0.2))

# Filter features where the information gain is not zero
over0.4_features <- mice::filter(weights, attr_importance > 0)$attr

# Use cutoff.biggest.diff() 
# This function returns the attributes that have a significantly higher information gain values than the others.
#cutoff.biggest.diff(weights)

```

```{r both sampling and 0.3 p-value}

# Use function information.gain to compute information gain values of the attributes
weights <- information.gain(Target ~ ., data = training_both0.3)

# Print weights
print(weights)

# add row names as a column to keep them during ordering
weights$attr  <- rownames(weights)

# Let's sort the weights in decreasing order of information gain values.
# We will use arrange() function 
weights <- arrange(weights, -weights$attr_importance) 

# Plot the weights
barplot(weights$attr_importance, names = weights$attr, las = 2, ylim = c(0, 0.2))

# Filter features where the information gain is not zero
both0.3_features <- mice::filter(weights, attr_importance > 0)$attr

# Use cutoff.biggest.diff() 
# This function returns the attributes that have a significantly higher information gain values than the others.
#cutoff.biggest.diff(weights)

```

```{r under sampling and 0.3 p-value}

# Use function information.gain to compute information gain values of the attributes
weights <- information.gain(Target ~ ., data = training_under0.3)

# Print weights
print(weights)

# add row names as a column to keep them during ordering
weights$attr  <- rownames(weights)

# Let's sort the weights in decreasing order of information gain values.
# We will use arrange() function 
weights <- arrange(weights, -weights$attr_importance) 

# Plot the weights
barplot(weights$attr_importance, names = weights$attr, las = 2, ylim = c(0, 0.2))

# Filter features where the information gain is not zero
under0.3_features <- mice::filter(weights, attr_importance > 0)$attr

# Use cutoff.biggest.diff() 
# This function returns the attributes that have a significantly higher information gain values than the others.
#cutoff.biggest.diff(weights)

```

```{r over sampling and 0.3 p-value}

# Use function information.gain to compute information gain values of the attributes
weights <- information.gain(Target ~ ., data = training_over0.3)

# Print weights
print(weights)

# add row names as a column to keep them during ordering
weights$attr  <- rownames(weights)

# Let's sort the weights in decreasing order of information gain values.
# We will use arrange() function 
weights <- arrange(weights, -weights$attr_importance) 

# Plot the weights
barplot(weights$attr_importance, names = weights$attr, las = 2, ylim = c(0, 0.2))

# Filter features where the information gain is not zero
over0.3_features <- mice::filter(weights, attr_importance > 0)$attr

# Use cutoff.biggest.diff() 
# This function returns the attributes that have a significantly higher information gain values than the others.
#cutoff.biggest.diff(weights)

```

Next, we will build some of the predictive models. Training data would be used to train and develop our predictive models. Later, we will use test data to predict whether a (lead) customer will convert and buy the new term deposit (1) or not (0).

#### LR models on RF Model with different data balancing method and proportion

Below we build LR models on the 9 different training dataset.

```{r LR model on the 0.5 balance ratio and both sampled training dataset}

# Construct a new training dataset containing important features only
training_both0.5_select <- training_both0.5[both0.5_features]
training_both0.5_select$Target <- training_both0.5$Target

# Build a logistic regression model assign it to LogReg
LogReg_both0.5 <- glm(Target ~ . , training_both0.5_select, family = "binomial")

# Predict the class probabilities of the test data
LogReg_both0.5_pred <- predict(LogReg_both0.5, test, type="response")

head(LogReg_both0.5_pred)

# Check the levels of target variable
levels(training_both0.5_select$Target)

# Predict the class 
LogReg_both0.5_class <- ifelse(LogReg_both0.5_pred > 0.5, 1, 0)

# Save the predictions as factor variables
LogReg_both0.5_class <- as.factor(LogReg_both0.5_class)

```

```{r LR model on the 0.5 balance ratio and under sampled training dataset}

# Construct a new training dataset containing important features only
training_under0.5_select <- training_under0.5[under0.5_features]
training_under0.5_select$Target <- training_under0.5$Target

# Build a logistic regression model assign it to LogReg
LogReg_under0.5 <- glm(Target ~ . , training_under0.5_select, family = "binomial")

# Predict the class probabilities of the test data
LogReg_under0.5_pred <- predict(LogReg_under0.5, test, type="response")

head(LogReg_under0.5_pred)

# Check the levels of target variable
levels(training_under0.5_select$Target)

# Predict the class 
LogReg_under0.5_class <- ifelse(LogReg_under0.5_pred > 0.5, 1, 0)

# Save the predictions as factor variables
LogReg_under0.5_class <- as.factor(LogReg_under0.5_class)

```

```{r LR model on the 0.5 balance ratio and over sampled training dataset}

# Construct a new training dataset containing important features only
training_over0.5_select <- training_over0.5[over0.5_features]
training_over0.5_select$Target <- training_over0.5$Target

# Build a logistic regression model assign it to LogReg
LogReg_over0.5 <- glm(Target ~ . , training_over0.5_select, family = "binomial")

# Predict the class probabilities of the test data
LogReg_over0.5_pred <- predict(LogReg_over0.5, test, type="response")

head(LogReg_over0.5_pred)

# Check the levels of target variable
levels(training_over0.5_select$Target)

# Predict the class 
LogReg_over0.5_class <- ifelse(LogReg_over0.5_pred > 0.5, 1, 0)

# Save the predictions as factor variables
LogReg_over0.5_class <- as.factor(LogReg_over0.5_class)

```

```{r LR model on the 0.4 balance ratio and both sampled training dataset}

# Construct a new training dataset containing important features only
training_both0.4_select <- training_both0.4[both0.4_features]
training_both0.4_select$Target <- training_both0.4$Target

# Build a logistic regression model assign it to LogReg
LogReg_both0.4 <- glm(Target ~ . , training_both0.4_select, family = "binomial")

# Predict the class probabilities of the test data
LogReg_both0.4_pred <- predict(LogReg_both0.4, test, type="response")

head(LogReg_both0.4_pred)

# Check the levels of target variable
levels(training_both0.4_select$Target)

# Predict the class 
LogReg_both0.4_class <- ifelse(LogReg_both0.4_pred > 0.5, 1, 0)

# Save the predictions as factor variables
LogReg_both0.4_class <- as.factor(LogReg_both0.4_class)

```

```{r LR model on the 0.4 balance ratio and under sampled training dataset}

# Construct a new training dataset containing important features only
training_under0.4_select <- training_under0.4[under0.4_features]
training_under0.4_select$Target <- training_under0.4$Target

# Build a logistic regression model assign it to LogReg
LogReg_under0.4 <- glm(Target ~ . , training_under0.4_select, family = "binomial")

# Predict the class probabilities of the test data
LogReg_under0.4_pred <- predict(LogReg_under0.4, test, type="response")

head(LogReg_under0.4_pred)

# Check the levels of target variable
levels(training_under0.4_select$Target)

# Predict the class 
LogReg_under0.4_class <- ifelse(LogReg_under0.4_pred > 0.5, 1, 0)

# Save the predictions as factor variables
LogReg_under0.4_class <- as.factor(LogReg_under0.4_class)

```

```{r LR model on the 0.4 balance ratio and over sampled training dataset}

# Construct a new training dataset containing important features only
training_over0.4_select <- training_over0.4[over0.4_features]
training_over0.4_select$Target <- training_over0.4$Target

# Build a logistic regression model assign it to LogReg
LogReg_over0.4 <- glm(Target ~ . , training_over0.4_select, family = "binomial")

# Predict the class probabilities of the test data
LogReg_over0.4_pred <- predict(LogReg_over0.4, test, type="response")

head(LogReg_over0.4_pred)

# Check the levels of target variable
levels(training_over0.4_select$Target)

# Predict the class 
LogReg_over0.4_class <- ifelse(LogReg_over0.4_pred > 0.5, 1, 0)

# Save the predictions as factor variables
LogReg_over0.4_class <- as.factor(LogReg_over0.4_class)

```

```{r LR model on the 0.3 balance ratio and both sampled training dataset}

# Construct a new training dataset containing important features only
training_both0.3_select <- training_both0.3[both0.3_features]
training_both0.3_select$Target <- training_both0.3$Target

# Build a logistic regression model assign it to LogReg
LogReg_both0.3 <- glm(Target ~ . , training_both0.3_select, family = "binomial")

# Predict the class probabilities of the test data
LogReg_both0.3_pred <- predict(LogReg_both0.3, test, type="response")

head(LogReg_both0.3_pred)

# Check the levels of target variable
levels(training_both0.3_select$Target)

# Predict the class 
LogReg_both0.3_class <- ifelse(LogReg_both0.3_pred > 0.5, 1, 0)

# Save the predictions as factor variables
LogReg_both0.3_class <- as.factor(LogReg_both0.3_class)

```

```{r LR model on the 0.3 balance ratio and under sampled training dataset}

# Construct a new training dataset containing important features only
training_under0.3_select <- training_under0.3[under0.3_features]
training_under0.3_select$Target <- training_under0.3$Target

# Build a logistic regression model assign it to LogReg
LogReg_under0.3 <- glm(Target ~ . , training_under0.3_select, family = "binomial")

# Predict the class probabilities of the test data
LogReg_under0.3_pred <- predict(LogReg_under0.3, test, type="response")

head(LogReg_under0.3_pred)

# Check the levels of target variable
levels(training_under0.3_select$Target)

# Predict the class 
LogReg_under0.3_class <- ifelse(LogReg_under0.3_pred > 0.5, 1, 0)

# Save the predictions as factor variables
LogReg_under0.3_class <- as.factor(LogReg_under0.3_class)

```

```{r LR model on the 0.3 balance ratio and over sampled training dataset}

# Construct a new training dataset containing important features only
training_over0.3_select <- training_over0.3[over0.3_features]
training_over0.3_select$Target <- training_over0.3$Target

# Build a logistic regression model assign it to LogReg
LogReg_over0.3 <- glm(Target ~ . , training_over0.3_select, family = "binomial")

# Predict the class probabilities of the test data
LogReg_over0.3_pred <- predict(LogReg_over0.3, test, type="response")

head(LogReg_over0.3_pred)

# Check the levels of target variable
levels(training_over0.3_select$Target)

# Predict the class 
LogReg_over0.3_class <- ifelse(LogReg_over0.3_pred > 0.5, 1, 0)

# Save the predictions as factor variables
LogReg_over0.3_class <- as.factor(LogReg_over0.3_class)

```

#### 9 LR Models Evaluation (Performance Metrics)

```{r Performance of 9 LR models with different sampling method and ratio}

# LR model on the 0.5 balance ratio and both sampled training dataset
confusionMatrix(LogReg_both0.5_class, test$Target, positive = "1", mode = "prec_recall")

# LR model on the 0.5 balance ratio and under sampled training dataset
confusionMatrix(LogReg_under0.5_class, test$Target, positive = "1", mode = "prec_recall")

# LR model on the 0.5 balance ratio and over sampled training dataset
confusionMatrix(LogReg_over0.5_class, test$Target, positive = "1", mode = "prec_recall")

# LR model on the 0.4 balance ratio and both sampled training dataset
confusionMatrix(LogReg_both0.4_class, test$Target, positive = "1", mode = "prec_recall")

# LR model on the 0.4 balance ratio and under sampled training dataset
confusionMatrix(LogReg_under0.4_class, test$Target, positive = "1", mode = "prec_recall")

# LR model on the 0.4 balance ratio and over sampled training dataset
confusionMatrix(LogReg_over0.4_class, test$Target, positive = "1", mode = "prec_recall")

# LR model on the 0.3 balance ratio and both sampled training dataset
confusionMatrix(LogReg_both0.3_class, test$Target, positive = "1", mode = "prec_recall")

# LR model on the 0.3 balance ratio and under sampled training dataset
confusionMatrix(LogReg_under0.3_class, test$Target, positive = "1", mode = "prec_recall")

# LR model on the 0.3 balance ratio and over sampled training dataset
confusionMatrix(LogReg_over0.3_class, test$Target, positive = "1", mode = "prec_recall")

```



### Different Training-Test Split

What percentage of the training data should we extract from the full dataset is most effective in obtaining a reasonable predictive accuracy/Precision/Recall/F1?

```{r 80 percent training data and 30 percent both sampling}
# Training Data Percentage: 80%

# index_80 keeps the record indices for the 80% training data
index_80 = createDataPartition(lead_data$Target, p = 0.8, list = FALSE)
# Generate training and test data
training_80 = lead_data[index_80,]
test_20 = lead_data[-index_80,]

# Apply both sampling techniques on the training_80
training_80 <- ovun.sample(Target ~ ., data = training_80, method = "both", p=0.3, seed=1)$data

# Use function information.gain to compute information gain values of the attributes
weights <- information.gain(Target ~ ., data = training_80)
# add row names as a column to keep them during ordering
weights$attr  <- rownames(weights)
# Let's sort the weights in decreasing order of information gain values.
# We will use arrange() function 
weights <- arrange(weights, -weights$attr_importance) 
# Plot the weights
#barplot(weights$attr_importance, names = weights$attr, las = 2, ylim = c(0, 0.2))

# Filter features where the information gain is not zero
training80_features <- mice::filter(weights, attr_importance > 0)$attr

# Construct a new training dataset containing important features only
training80_select <- training_80[training80_features]
training80_select$Target <- training_80$Target

# Build a logistic regression model
LogReg_training80 <- glm(Target ~ . , training80_select, family = "binomial")

# Predict the class probabilities of the test data
LogReg_training80_pred <- predict(LogReg_training80, test_20, type="response")

#head(LogReg_training80_pred)

# Check the levels of target variable
levels(training80_select$Target)

# Predict the class 
LogReg_training80_class <- ifelse(LogReg_training80_pred > 0.5, 1, 0)

# Save the predictions as factor variables
LogReg_training80_class <- as.factor(LogReg_training80_class)

# LR Model Performance on 80% training data
confusionMatrix(LogReg_training80_class, test_20$Target, positive = "1", mode = "prec_recall")
```

```{r 70 percent training data and 30 percent both sampling}
# Training Data Percentage: 70%

# index_70 keeps the record indices for the training data 
index_70 = createDataPartition(lead_data$Target, p = 0.7, list = FALSE)
# Generate training and test data 
training_70 = lead_data[index_70,]
test_30 = lead_data[-index_70,]

# Apply both sampling technique on the training_70
training_70 <- ovun.sample(Target ~ ., data = training_70, method = "both", p=0.3, seed=1)$data

# Use function information.gain to compute information gain values of the attributes
weights <- information.gain(Target ~ ., data = training_70)
# add row names as a column to keep them during ordering
weights$attr  <- rownames(weights)
# Let's sort the weights in decreasing order of information gain values.
# We will use arrange() function 
weights <- arrange(weights, -weights$attr_importance) 
# Plot the weights
#barplot(weights$attr_importance, names = weights$attr, las = 2, ylim = c(0, 0.2))

# Filter features where the information gain is not zero
training70_features <- mice::filter(weights, attr_importance > 0)$attr

# Construct a new training dataset containing important features only
training70_select <- training_70[training70_features]
training70_select$Target <- training_70$Target

# Build a logistic regression model
LogReg_training70 <- glm(Target ~ . , training70_select, family = "binomial")

# Predict the class probabilities of the test data
LogReg_training70_pred <- predict(LogReg_training70, test_30, type="response")
#head(LogReg_training70_pred)

# Check the levels of target variable
levels(training70_select$Target)

# Predict the class 
LogReg_training70_class <- ifelse(LogReg_training70_pred > 0.5, 1, 0)

# Save the predictions as factor variables
LogReg_training70_class <- as.factor(LogReg_training70_class)

# LR Model Performance on 70% training data
confusionMatrix(LogReg_training70_class, test_30$Target, positive = "1", mode = "prec_recall")
```

```{r 60 percent training data and 30 percent both sampling}
# Training Data Percentage: 60%

# index_60 keeps the record indices for the training data
index_60 = createDataPartition(lead_data$Target, p = 0.6, list = FALSE)
# Generate training and test data
training_60 = lead_data[index_60,]
test_40 = lead_data[-index_60,]

# Apply both sampling technique on the training_60
training_60 <- ovun.sample(Target ~ ., data = training_60, method = "both", p=0.3, seed=1)$data

# Use function information.gain to compute information gain values of the attributes
weights <- information.gain(Target ~ ., data = training_60)
# add row names as a column to keep them during ordering
weights$attr  <- rownames(weights)
# Let's sort the weights in decreasing order of information gain values.
# We will use arrange() function 
weights <- arrange(weights, -weights$attr_importance) 
# Plot the weights
#barplot(weights$attr_importance, names = weights$attr, las = 2, ylim = c(0, 0.2))

# Filter features where the information gain is not zero
training60_features <- mice::filter(weights, attr_importance > 0)$attr

# Construct a new training dataset containing important features only
training60_select <- training_60[training60_features]
training60_select$Target <- training_60$Target

# Build a logistic regression model
LogReg_training60 <- glm(Target ~ . , training60_select, family = "binomial")

# Predict the class probabilities of the test data
LogReg_training60_pred <- predict(LogReg_training60, test_40, type="response")
#head(LogReg_training60_pred)

# Check the levels of target variable
levels(training60_select$Target)

# Predict the class 
LogReg_training60_class <- ifelse(LogReg_training60_pred > 0.5, 1, 0)

# Save the predictions as factor variables
LogReg_training60_class <- as.factor(LogReg_training60_class)

# LR Model Performance on 60% training data
confusionMatrix(LogReg_training60_class, test_40$Target, positive = "1", mode = "prec_recall")
```

The optimal LR model should be built on the training data set using the CART in MICE Imputation method. Also, we should adopt the both sampling method (p = 0.3) and set p = 0.7 for the training dataset. 

### Information Gain

```{r Compute information gain}

# index keeps the record indices for the training data
index = createDataPartition(lead_data$Target, p = 0.7, list = FALSE)
training = lead_data[index,]
test = lead_data[-index,]

# Apply both sampling technique on the training data
training <- ovun.sample(Target ~ ., data = training, method = "both", p=0.3, seed=1)$data

# Use function information.gain to compute information gain values of the attributes
weights <- information.gain(Target ~ ., data = training)

# Print weights
print(weights)

# add row names as a column to keep them during ordering
weights$attr  <- rownames(weights)

# Let's sort the weights in decreasing order of information gain values.
# We will use arrange() function 
weights <- arrange(weights, -weights$attr_importance) 

# Plot the weights
par(mar=c(8,4,0.5,2))
barplot(weights$attr_importance, names = weights$attr, las = 2, ylim = c(0, 0.2), cex.names = 0.83, cex.lab = 0.75)

# Filter features where the information gain is not zero
features <- mice::filter(weights, attr_importance > 0)$attr

# Use cutoff.biggest.diff() 
# This function returns the attributes that have a significantly higher information gain values than the others.
cutoff.biggest.diff(weights)

```
 
```{r Visualization for Registration and Credit_Product}

# Visualization for Registration
ggplot(training, 
      aes(x = Target, group = Registration)) + 
      geom_bar(aes(y = after_stat(prop), fill = factor(after_stat(x))), 
                   stat="count", 
                   alpha = 0.7) +
      geom_text(aes(label = scales::percent(after_stat(prop)), y = after_stat(prop) ), 
                   stat= "count", 
                   vjust = -.1) +
      labs(y = "Percentage") +
      facet_grid(~Registration) +
      scale_fill_manual("Target" ,values = c("steelblue","orange"), labels=c("No", "Yes")) + 
      theme(plot.title = element_text(hjust = 0.5)) + 
      ggtitle("Registration")

# Visualization for Credit_Product
ggplot(training, 
      aes(x = Target, group = Credit_Product)) + 
      geom_bar(aes(y = after_stat(prop), fill = factor(after_stat(x))), 
                   stat="count", 
                   alpha = 0.7) +
      geom_text(aes(label = scales::percent(after_stat(prop)), y = after_stat(prop) ), 
                   stat= "count", 
                   vjust = -.1) +
      labs(y = "Percentage") +
      facet_grid(~Credit_Product) +
      scale_fill_manual("Target" ,values = c("steelblue","orange"), labels=c("No", "Yes")) + 
      theme(plot.title = element_text(hjust = 0.5)) + 
      ggtitle("Credit Product")

```


### Best LR Model

```{r building LR models on the MICE Imputed both-sampled 30 percent dataset}

# Construct a new training dataset containing important features only
training_select <- training[features]
training_select$Target <- training$Target

# Build a logistic regression model assign it to LogReg
LogReg <- glm(Target ~ . , training_select, family = "binomial")

# Predict the class probabilities of the test data
LogReg_pred <- predict(LogReg, test, type="response")

head(LogReg_pred)

# Check the levels of target variable
levels(training_select$Target)

# Predict the class 
LogReg_class <- ifelse(LogReg_pred > 0.5, 1, 0)

# Save the predictions as factor variables
LogReg_class <- as.factor(LogReg_class)

# Optimal LR Model Performance
confusionMatrix(LogReg_class, test$Target, positive = "1", mode = "prec_recall")

```


## Support Vector Machine (SVM) Model

```{r Construct training data for SVM model}

# Only top 5 features selected for building SVM model due to high computational time
svm_lead_data <- lead_data[c("Registration","Credit_Product","Age", "Channel_Code", "Vintage","Target")]

# Perform one-hot encoding
svm_lead_data <- one_hot(as.data.table(svm_lead_data), cols = c("Registration","Credit_Product", "Channel_Code"))
```

```{r Construct 70 percent training data for SVM model}

# index_RF keeps the record indices for the training data
index_svm = createDataPartition(svm_lead_data$Target, p = 0.7, list = FALSE)

# Generate training and test data 
training_svm = svm_lead_data[index_svm,]
test_svm = svm_lead_data[-index_svm,]
```

```{r apply both sampling method on the traning data for SVM model}
training_svm <- ovun.sample(Target ~ ., data = training_svm, method = "both", p=0.3, seed=1)$data
```

```{r building SVM models on the RF Imputed training dataset}
# Build an SVM model by using svm() function
svm_model <- svm(Target ~. , data = training_svm, kernel = "radial", scale = TRUE, probability = TRUE)

# Print svm_model
print(svm_model)

# Predict the Test set results 
svm_predict = predict(svm_model, test_svm, probabilities = TRUE)

# Copy test data to results
results_svm <- test_svm

# The following code generates a column named PredictionSVM in dataframe results and adds predictions obtained by SVM to that column
results_svm$PredictionSVM <-  svm_predict

# SVM Model Performance
confusionMatrix(svm_predict, test_svm$Target, positive='1', mode = "prec_recall")
```

## Decision Tree Model

```{r optimal DT model}
# building model
DT_model <- C5.0(Target~., data = training_select)

# applying the DT model on test data
DT_predict <- predict(DT_model, test, type = "class")

# Model performance of the optimal DT model
confusionMatrix(DT_predict, test$Target, positive = "1", mode = "prec_recall")
```

## Random Forest Model

```{r optimal RF model}
set.seed(123)

# Building optimal RF model
RF_model <- randomForest(Target~., training_select)

# Evaluating the variable importance in the RF model
importance(RF_model)
varImpPlot(RF_model)

# applying the RF model on test data
RF_predict <- predict(RF_model, test)

# Model performance of the optimal RF model
confusionMatrix(RF_predict, test$Target, positive = "1", mode = "prec_recall")

```

# Model Evaluation
## ROC
```{r ROC}
# Preparing the ROC for LR model
ROC_LogReg <- roc(test$Target, LogReg_pred)

# Preparing the ROC for SVM model
SVMpred <- predict(svm_model, test_svm, probability = TRUE)
svm_prob <- attr(SVMpred, "probabilities")
ROC_svm <- roc(test_svm$Target, svm_prob[,2])

# Preparing the ROC for RF model
RF_prob <- predict(RF_model, test, type = "prob")
ROC_RF <- roc(test$Target, RF_prob[,2])

# Plot the ROC curve for Logistic Regression, SVM and Random Forest
pROC::ggroc(list(LogReg = ROC_LogReg, SVM = ROC_svm, RF = ROC_RF), legacy.axes=TRUE)+ xlab("FPR") + ylab("TPR") +
   geom_abline(intercept = 0, slope = 1, color = "darkgrey", linetype = "dashed") +
  theme_light() + 
  theme(panel.grid.major.y = element_blank(), panel.grid.minor.y = element_blank())

```
## AUC
```{r Area Under Curve (AUC)}
# Compute AUC for each model
auc(ROC_LogReg)
auc(ROC_svm)
auc(ROC_RF)
```

## Cumulative Gain Chart

Plot the gain chart with increment of 10/100.

```{r Preparing the curves for each model}

library(CustomerScoringMetrics)

# Provide probabilities for the outcome of interest and obtain the gain chart data

GainTable_LogReg <- cumGainsTable(LogReg_pred, test$Target, resolution = 1/100)

GainTable_SVM <- cumGainsTable(svm_prob[,2], test_svm$Target, resolution = 1/100)

GainTable_RF <- cumGainsTable(RF_prob[,2], test$Target, resolution = 1/100)


```

```{r Plot cumulative gain chartfig.width=5, fig.height=5}

# Add the initial points of the curves
zero_gain <-data.frame(0,0,0,0)
zero_gain <- data.matrix(zero_gain)
GainTable_LogReg <- rbind(zero_gain, GainTable_LogReg)
GainTable_RF <- rbind(zero_gain, GainTable_RF)
GainTable_SVM <- rbind(zero_gain, GainTable_SVM)

# Percentage of identified converted leads (customers)

plot(GainTable_RF[,4], col="steelblue", type ="l",
xlab="% Test Instances (Data)", ylab="% Correct Predictions", xaxt = "n", yaxt = "n")
lines(GainTable_SVM[,4], col="forestgreen", type ="l")
lines(GainTable_LogReg[,4], col="darkred", type="l")
axis(1, xaxp=c(0, 100, 10), las=1)
axis(2, yaxp=c(0, 100, 10), las=2)
abline(0, 1, lty=2, col = "black")
abline(h = seq(0, 100, 10), col = 'darkgray', lty = 3)
abline(v = seq(0, 100, 10), col = 'darkgray', lty = 3)
#grid(nx = 5, ny = 5, lwd = 2)

legend("bottomright",
c("RF Model", "SVM Model", "LogReg Model", "Baseline"),
fill=c("steelblue", "forestgreen", "darkred", "black"))

```

# Tuning the RF Models
```{r}
set.seed(123)

# Perform joint hyperparameter tuning using tune function
tuned_RF_parameters <- randomForestSRC::tune(Target~., training_select,
  mtryStart = sqrt(ncol(training_select)),   
  nodesizeTry = seq(1, 10, by = 2), 
  ntree = 500,
  stepFactor = 1.25, improve = 0.001)

# View the results to see the best hyperparameters
tuned_RF_parameters$optimal

set.seed(123)

# Build tuned RF model
tuned_RF_model <-  randomForest(Target~., training_select, mtry = 6, nodesize = 1)

# Apply tuned RF model on test data
tuned_RF_pred <- predict(tuned_RF_model, test)

# Model performance of the tuned RF model
confusionMatrix(tuned_RF_pred, test$Target, positive='1', mode = "prec_recall")

```

# Appendix 

##Data Visualization

### Registration

```{r Registration bar plot, fig.width=10, fig.height=7}

training_visual <- training
str(training_visual)

training_visual$Registration <- recode(training_visual$Registration, "0" = "No", "1" = "Yes")

training_visual$Credit_Product <- recode(training_visual$Credit_Product, "0" = "No", "1" = "Yes")

training_visual$Target <- recode(training_visual$Target, "0" = "No", "1" = "Yes")

str(training_visual)

# Visualization for Registration
ggplot(training_visual, 
      aes(x = Target, group = Registration)) +
  theme(text = element_text(size=20)) +
      geom_bar(aes(y = after_stat(prop), fill = factor(after_stat(x))), 
                   stat="count", 
                   alpha = 0.7) +
      geom_text(aes(label = scales::percent(after_stat(prop)), y = after_stat(prop) ), 
                   stat= "count", 
                   vjust = -.1) +
      labs(y = "Percentage") +
      facet_grid(~Registration) +
      scale_fill_manual("Target" ,values = c("steelblue","orange"), labels=c("No", "Yes")) + 
      theme(plot.title = element_text(hjust = 0.5)) + 
      ggtitle("Registration")


```

### Credit Product

```{r Credit Product bar plot, fig.width=10, fig.height=7}
# Visualization for Credit_Product
ggplot(training_visual, 
      aes(x = Target, group = Credit_Product)) + 
    theme(text = element_text(size=20)) +
      geom_bar(aes(y = after_stat(prop), fill = factor(after_stat(x))), 
                   stat="count", 
                   alpha = 0.7) +
      geom_text(aes(label = scales::percent(after_stat(prop)), y = after_stat(prop) ), 
                   stat= "count", 
                   vjust = -.1) +
      labs(y = "Percentage") +
      facet_grid(~Credit_Product) +
      scale_fill_manual("Target" ,values = c("steelblue","orange"), labels=c("No", "Yes")) + 
      theme(plot.title = element_text(hjust = 0.5)) + 
      ggtitle("Credit Product")

```

### Age

```{r put all customer ages into age groups, fig.width=10, fig.height=7}

# add another age group variable into the both sampled training data
training_visual_age <- training_visual %>% 
  mutate(
    # Create categories
    Age_Group = dplyr::case_when(
      Age <= 40            ~ "<40",
      Age > 40 & Age <= 60 ~ "40-60",
      Age > 60 & Age <= 80 ~ "60-80",
      Age > 80             ~ ">80"
    ),
    # Convert to factor
    Age_Group = factor(
      Age_Group,
      level = c("<40", "40-60","60-80", ">80")
    )
  ) 
  
```

```{r Age distribution histogram, fig.width=10, fig.height=7}

ggplot(lead_data_raw, aes(x=Age)) +
    theme_light() + 
    theme(text = element_text(size=20)) +
    geom_histogram(binwidth=.5, colour="steelblue", fill="steelblue")
```

```{r Age group bar plot, fig.width=10, fig.height=7}
# Visualization for Credit_Product
ggplot(training_visual_age, 
      aes(x = Target, group = Age_Group)) + 
      theme(text = element_text(size=20)) +
      geom_bar(aes(y = after_stat(prop), fill = factor(after_stat(x))), 
                   stat="count", 
                   alpha = 0.7) +
      geom_text(aes(label = scales::percent(after_stat(prop)), y = after_stat(prop) ), 
                   stat= "count", 
                   vjust = -.1) +
      labs(y = "Percentage") +
      facet_grid(~Age_Group) +
      scale_fill_manual("Target" ,values = c("steelblue","orange"), labels=c("No", "Yes")) + 
      theme(plot.title = element_text(hjust = 0.5)) + 
      ggtitle("Age Group")

```
